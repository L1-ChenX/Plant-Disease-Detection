\newcommand{\fig}[3]{
	\begin{figure}[!htb]
		\centering
		\includegraphics[width=#2\textwidth]{#1.png}
		\caption{#3}
		\label{#1-en}
	\end{figure}
}
\newcommand{\tab}[3]{
	\begin{table}[!htb]
		\centering
		\caption{#3} 
		\includegraphics[width=#2\textwidth]{#1.png}
		\label{#1-en}
	\end{table}
}


\tnr

\begin{center}
    \zihao{4}\textbf{Plant Disease Identification Using a Novel Convolutional Neural Network}\\    
    \zuozhe SK MAHMUDUL HASSAN AND ARNAB KUMAR MAJI , (Member, IEEE)\\
    \xuexiao Department of Information Technology, School of Technology, North-Eastern Hill University (NEHU), \\
    Shillong 793022, India\\
    Corresponding author: Arnab Kumar Maji (arnab.maji@gmail.com)\\
\end{center}

\fontsize{10.5pt}{10.5pt}\selectfont

\noindent\textbf{Abstract: }The timely identification of plant diseases prevents the negative impact on crops. Convolutional neural network, particularly deep learning is used widely in machine vision and pattern recognition task. Researchers proposed different deep learning models in the identification of diseases in plants. However, the deep learning models require a large number of parameters, and hence the required training time is more and also difficult to implement on small devices. In this paper, we have proposed a novel deep learning model based on the inception layer and residual connection. Depthwise separable convolution is used to reduce the number of parameters. The proposed model has been trained and tested on three different plant diseases datasets. The performance accuracy obtained on plantvillage dataset is 99.39\%, on the rice disease dataset is 99.66\%, and on the cassava dataset is 76.59\%. With fewer number of parameters, the proposed model achieves higher accuracy in comparison with the state-of-art deep learning models.

\noindent\textbf{Index Terms: }Plant disease, machine learning, deep learning, depthwise convolution, pointwise convolution.

\section{Introduction} \label{s1-en}
Diseases in crops caused mainly by bacteria and fungi negatively impact the production and quality of crops . Timely and early identification of disease symptoms is a major challenge in protecting crops. Visual identification of diseases in a large farm by experts and agronomists is the primary approach in developing countries which is time-consuming and costly. Automatic identification of diseases using smart devices is a promising approach to identifying and reducing overall costs . In recent times, deep learning particularly convolution neural network (CNN) gained much attention in agricultural field such as plant detection , fruit detection , disease identification –, weeds detection , pest recognition  etc. The reason behind the CNN-based model’s popularity is the automatic extraction of appropriate features from the data set. Several popular deep learning-based models such as AlexNet , GoogleNet , VGGNet , ResNet , DenseNet  have been developed for identification of plant diseases.

Real-time applications and the identification of diseases using deep learning architectures are gaining attention in the current scenario. The number of parameters used and the computation cost in the deep learning models depend on the depth and the number of filters used in the model. The deep learning models usually require a large number of parameters. So the computational cost of the deep learning models is also high. Therefore there will be difficulties to implement on small devices having small resources . In recent times, researchers have implemented deep learning architectures using high-powered devices having GPUs and servers. Uses of sophisticated devices having GPU’s are not feasible in the agricultural field due to its high cost. Therefore there is a dearth need for applications having fewer parameters, less power consumption, and computation .

In prospect of the above consideration, we have designed a novel lightweight deep learning model to identify the diseases in the plant. This paper proposes a novel CNN architecture based on Inception and ResNet with fewer parameters for determining plant diseases. The Inception architecture extracts better features using multiple convolutions of different filter sizes. To tackle the vanishing gradient problem, we have used residual connections. Instead of standard convolution, we have used depth-wise separable convolution, which reduces the parameter size and the computational complexity without affecting performances. The model has trained on three different datasets, and the performances are evaluated. The main contribution of the paper is summarized as follows:

A new CNN architecture is proposed using Inception and Residual connection, which extracts better features and produces higher performance results.

In this paper, the standard convolution is replaced with depth-wise separable convolution, which reduces the parameter number by a large margin without affecting performances.

The proposed architecture uses fewer parameters than other deep learning architecture and is also faster than the standard deep learning models.

To check the robustness, the model’s performance is evaluated on three different plant disease data sets. We have considered three different conditioned images. In the PlantVillage dataset, the images were captured on uniform background and laboratory setup conditions. The images in the rice disease dataset were captured in real-time field conditions, and in the cassava plant dataset, the images are field conditioned, and images contain multiple leaves. We have compared the performance of the proposed model with other state-of-art deep learning models on three different datasets. The result shows that our proposed model outperforms other deep learning models.

The rest of the paper is organized as follows: Section \ref{s2-en} provides the existing literature on the identification of plant diseases using deep learning models. Materials and methods are discussed in Section \ref{s3-en} Section \ref{s4-en} presents the experimental results and discussions. Finally, the paper concludes with Section \ref{s5-en}.

\section{Related Work} \label{s2-en}
This section presents a discussion and review of recent work on identifying plant diseases based on deep learning models. Mohanty et al.  used AlexNet and GoogleNet to identify 26 diseases of 14 different plant species. To train the models they have used both transfer learning and learning from the scratch method. They achieved a maximum accuracy of 99.34\% using GoogleNet. Five different pre-trained deep learning models, such as VGG, AlexNet, AlexNetOWTBn, Overfeat, and GoogleNet used by Ferentinos  to identify 58 plant leaf diseases. Geetharamani and Pandian  used nine-layer deep CNN to identify the diseases in plants and achieved an accuracy rate of 96.46\%. Inspired from AlexNet and GoogLeNet architecture, Liu et al.  designed a model by replacing the fully connected layer of AlexNet with the inception layer to identify four different apple leaf diseases and recorded an accuracy rate of 97.62\%.

Four different pre-trained deep learning architecture namely VGG16, VGG19, ResNet, and InceptionV3 used by Ahmad et al.  to identify different tomato leaf diseases. They fine-tuned the network parameter to get the optimal result. They achieved the highest performance accuracy using InceptionV3 is 99.60\% and 93.70\% on laboratory and field images respectively. Rangarajan and Purushothaman  used a pre-trained VGG16 model to identify different eggplant diseases. VGG16 was used for the extraction of features. For classification, they used multiclass SVM. To check the robustness of the model performances, they have used three different color model images, namely RGB, YCbCr and HSV. Using RGB images, they recorded maximum accuracy of 99.4\%.

Too et al.  used several pre-trained deep learning models and fine-tuned the model parameter in the identification and classification of diseases in the plant. They have achieved maximum testing accuracy of 99.75\% using DenseNet architecture. Deep feature-based and SVM classifier used by Sethy et al.  to identify rice leaf diseases. They have used 11 deep CNN models to extract the features and SVM for classification purposes. Deep features from the ResNet50 model with SVM perform better with an f1-score of 98.38\%. Six different pre-trained deep learning architecture used by Rangarajan Aravind and Raja  to identify 10 different diseases of four varieties of plants. Among the architectures, VGG16 gives the highest performance accuracy of 90\% on the test dataset.

Ramacharan et al.  used transfer learning on InceptionV3 to identify three diseases and two pest damages on cassava plants. The dataset used consists of leaves with multiple numbers of leaves in a single image. The accuracy obtained in the dataset having single leaf images is higher than the images with multiple leaves. Later on, Ramacharan et al.  used mobile devices to identify six different diseases in the cassava plant. They have used MobileNet deep learning architecture to train the network. They have used both image and video files of diseased leaf images to evaluate the performance. They achieved an accuracy of 80.6\% and 70.4\% on the image file and video files respectively. Oyewola et al.  shown that deep learning with residual connection outperforms plain convolution neural network in the identification of different diseases in cassava plants.

Deep residual neural network with 50 layers along with batch normalization and ReLU activation after each convolution used by Picon et al.  to identify 3 different wheat diseases. They recorded an accuracy rate of 96\% on real-time field images captured in Germany. Durmus et al.  used SqueezeNet architecture to identify different tomato leaf diseases. The structure of SqueezeNet is similar to AlexNet architecture, whose size is 227.6MB, while the size of SequenzeNet is 2.9MB. Modified Cifar10 quick CNN model used by Hu et al.  and replaced the standard convolution with depthwise separable convolution to identify four different tea leaf diseases. They achieved an improved accuracy rate of 92.5\%. Atila et al.  used EfficientNet architecture to identify diseases in plants and shown that EfficientNet outperforms the standard CNN model such as AlexNet, VGG, etc., with an accuracy rate of 99.97\% using EfficientNetB4 on PlantVillage dataset. The EfficientNet model takes less time to train the network as the network uses fewer parameters compared to other deep learning models.

Pre-trained VGG model with inception layer termed as INC-VGGN used by Chen et al.  to identify the different corn and rice diseases. They replaced the fully connected layers of VGGNet and added two inception layers. The average testing accuracy obtained in rice diseases is 92\% and 80.38\% in corn diseases. Shallow CNN from the pre-trained VGG16 model used by Li et al.  to identify different diseases in corn, apple, and grape. They have used only the first four-layer of VGG16 architecture a global pooling layer. Shallow CNN was used to extract the features and PCA to reduce the dimensions. They obtained an f1-score of 94\% using SVM and RF respectively. Self-attention CNN was used by Zeng and Li  to identify different crop diseases. Attention network is effective in extracting the image features from the critical region. Table \ref{t1-1-en} summarizes the related works along with the deep learning models and performance results.

\tab{t1-1}{0.7}{Summary of Related Works on Identification of Plant Diseases}

\section{Materials and Methods} \label{s3-en}
\subsection{Convolution Neural Network}
Convolutional Neural Network (CNN) is a type of Neural network that is very effective in several computer vision tasks such as pattern recognition and classification. CNN has an advantage that it can learn and extracts the features automatically from the training images whereas in traditional approach there is a need of manual extraction of feature from the images. CNN consists of different layers: Convolutional layer, pooling layer, and fully connected layer. The convolutional layer is the primary and most crucial component of CNN which extracts the feature from the input image. Convolution layers consist of a small array of numbers called kernels applied over the input which produces an output called feature maps. Different convolutional kernels are used to extract different types of features. Several convolutional layers are there, which depend on the size of the input images.

After the convolutional layer, pooling is performed which is responsible for reducing the dimension of the convolutional feature map. The pooling layer performs down sampling operations by reducing the dimension of the feature map, which ultimately helps in reducing the required computational complexity to process the data. Different types of pooling operations are there, such as max-pooling, min-pooling, average-pooling. The output feature maps of the convolution or pooling layer are transformed into a one-dimensional vector in which every input is connected to every output by weight. This layer is also called a dense layer. There can be one or more fully connected layers, and the final fully connected layer has the same number output as the number of classes.

\subsection{Residual Network}
The convolutional neural network may achieve high performances in classification tasks. With the rise in network depth, the performance accuracy gets saturated and degrades rapidly. To address this issue in 2015, He et al.  introduced a deep residual learning network known as ResNet. In deep learning, using the residual connection in the network, we can train a large network and parallely solve the vanishing gradient problem, which usually occurs due to the increase in network depth. Figure \ref{f1-10-en} shows the basic block diagram of the ResNet model. ResNet introduced a skip connection known as “identity mapping” which combines previous layer’s output with the forthcoming layer. To perform identity mapping, it doesn’t generate any parameter. Therefore ResNet has the ability to train a deeper network with lower complexity than other networks such as VGG.

\fig{f1-1}{0.5}{Basic block diagram of residual network}

\subsection{Depthwise Separable Convolution}
Depthwise separable convolution introduced by Chollet  in the Xception model. Later on, Howard et al. used depthwise separable convolution in MobileNet architecture. Depthwise separable convolution factorizes standard convolution into depthwise convolution and 1×1 pointwise convolution. In standard convolution, it takes one step to filter the input images and combine these values. Figure \ref{f1-2-en} shows the block diagram of depthwise separable convolution. The Depthwise layer performs filtering operation, and the pointwise layer combines the output of the depthwise layer. The computation cost of depthwise separable convolution is computed as 

\begin{equation*} D_{K}^{2}\times M\times D_{F}^{2}+M\times N\times D_{K}^{2}\tag{1}\end{equation*}

whereas the computation cost in standard convolution is –

\begin{equation*} D_{K}^{2}\times M\times N \times D_{F}^{2}\tag{2}\end{equation*}

where DF is the dimension of the input image of square size, DK is the dimension of the kernel, M is the number of channels and N is the number of kernel/filters.

\fig{f1-2}{0.5}{Depthwise separable convolution}

\subsection{Proposed Novel CNN Approach for Identification of Plant Diseases}
In this paper, we have proposed a novel light weight CNN based on Inception and Residual connections with fewer parameters than the InceptionV3, ResNet50 as well as other deep learning models. The Inception in deep learning architecture was introduced by Szegedy et al. [11] in 2015 and named as GoogleNet (Inception-v1). Later on, additional factorization is added in the inception network and referred it as Inception-v3. Inception architecture is different from typical convolutional architecture and performs multiple convolutions and pooling operations simultaneously to extract the better feature. It concatenates the outputs of all the convolutions of different filter sizes. Inception v3 consist of numbers of Inception A, reduction A, Inception B and Reduction B blocks as shown in Figure \ref{f1-3-en}(a) and Figure \ref{f1-4-en}(a). The inception A block consist of a convolution layer with kernel size 1×1 , 1×1 convolution followed by 3×3 convolutions, 1×1 convolution followed by 5×5 convolutions, 3×3 max-pooling layer followed by 1×1 convolution. Inception B block consists of convolution layer with kernel size 1×1 , 1×1 convolution followed by 7×7 convolutions, 1×1 convolution followed by two 7×7 convolutions, 3×3 max-pooling layer followed by 1×1 convolution. 1×1 convolutions were used to reduce the computation of the model. The reduction A block uses a 3×3 max-pooling layer, 3×3 convolution layer, and 1×1 convolution layer followed by a 3×3 convolution layer. The reduction B block of inceptionv3 uses a 3×3 max-pooling layer, 3×3 convolution layer followed by 1×1 convolution layer, and 7×7 convolution layer followed by 3×3 convolution layer. Figure \ref{f1-3-en}(a) and Figure \ref{f1-4-en}(a) represent the block diagram of inception-A and inception-B block of inception-V3 architecture.

\fig{f1-3}{0.7}{Inception architecture of (a) Original inception-A block (b) Modified inception-A block}

\fig{f1-4}{0.7}{Inception architecture of (a) Original inception-B block (b) Modified inception-B block}

In our work, we have proposed a novel CNN model which uses the inception architecture with normal convolution as well as depthwise separable convolutions. In the proposed model, 3×3 convolution of Inception A block is replaced by 3×3 depthwise separable convolution (3×3 depthwise convolution and 1×1 pointwise convolution), and 5×5 convol- ution is replaced with two 3×3 depthwise separable convolutions as shown in Figure \ref{f1-3-en}(b). The 7×7 convolution used in Inception B is replaced with 7×7 depthwise separable convolution (7×7 depthwise convolution and 1×1 pointwise convolution) as shown in Figure \ref{f1-4-en}(b). The 3×3 convolution layer of reduction A replaced with a 1×1 convolution layer followed by 3×3 depthwise separable convolution. In reduction B block, we have replaced the 3×3 and 7×7 convolution with a 1×1 convolution layer and a 3×3 depthwise separable convolution layer. Table \ref{t1-2-en} and Table \ref{t1-3-en} shows the parameter comparison between the original inception-A block and the modified inception-A block with depthwise separable convolution. From Table \ref{t1-2-en} and Table \ref{t1-3-en} it is seen that the parameter used in the modified inception-A block is much less as compared to the original inception-A blocks.

\tab{t1-2}{0.7}{Parameter Required in Original Inception-A Block}

\tab{t1-3}{0.7}{Parameters Required in Modified Inception-A Block With Depth-Wise Separable Convolution}

Figure \ref{f1-5-en}. shows the proposed CNN architecture used to identify the diseases in plants. The proposed implemented model consists of a convolution layer, batch normalization and activation layer, inception blocks with depthwise separable convolution layer, pooling layer, and fully connected layer. In this architecture, we have replaced the standard convolution with depthwise separable convolution. We have used one standard convolution, three depthwise separable convolutions, two max-pooling, and one global average pooling operation, three modified inception A block with residual connection followed by modified reduction A block, three modified inception B block with residual connection followed by modified reduction B block. After each convolution layer, we have used Batch Normalization and activation. The activation function used is ReLu. Batch Normalization and activation function improves the performance and speeds up the process. After global average pooling, we have used dropout, which reduces the chances of overfitting the model. The required parameter in our proposed model is 428,100 while the number of parameter used in standard inceptionv V3 model is 23,851,784 which is much higher than the proposed model. It is observed that the proposed model uses 70\% less parameter as compared to the inception V3 architecture.

\fig{f1-5}{0.5}{Proposed CNN approach in identification of plant diseases}

\section{Results and Discussion} \label{s4-en}
\subsection{Dataset}
In this paper, we consider three different plant disease datasets to evaluate the performances of the model. We have used the Rice plant dataset [36], cassava plant dataset [25], and Plant village dataset [5] which is the large and available plant disease dataset. From the Plant village dataset specifically, we have used the corn, potato, and tomato plant diseases. It is seen that the images in the PlantVillage dataset are captured in a uniform background, and the intensity is also relatively uniform. The rice plant dataset consists of 5932 images which are divided into four categories, namely 1584 bacterial blight, 1440 blast, 1600 brown spot, and 1308 tungro images. The cassava plant dataset consists of 5,656 images includes five different categories as 316 healthy cassava leaf images and four sets of diseased leaves as 466 cassava bacteria blight, 1,443 cassava brown streak, 773 cassava green mite, and 2,658 cassava mosaic diseased images. The images in the cassava dataset are captured in the complex background, and multiple leaves are also present in a single image. The datasets are divided randomly into training and testing set in the ratio of 80:20. The dimensions of the images are resized into 256×256 pixels. Figure \ref{f1-6-en} shows the sample images of different datasets. Table \ref{t1-4-en}, Table \ref{t1-5-en}, Table \ref{t1-6-en} shows the dataset details along with the disease common name, disease scientific name, Type of disease along with the number of images in each class.

\tab{t1-4}{0.7}{Data Description of Plantvillage Dataset (Corn, Potato, Tomato)}

\tab{t1-5}{0.7}{Data Description of Rice Disease Data}

\tab{t1-6}{0.7}{Data Description of Cassava Dataset}

\tab{f1-6}{0.5}{Sample images of Plantvillage dataset, rice and cassava plant dataset}

\subsection{Experimental Result}
To evaluate the performance of the model, we consider different performance statistics such as number of parameter, accuracy, precision, recall, f1-score and expressed as follows-

\begin{align*} Accuracy=&\frac {TP+TN}{TP+FP+TN+FN} \tag{3}\\ Precision=&\frac {TP}{TP+FP} \tag{4}\\ Recall=&\frac {TP}{TP+FN} \tag{5}\\ f1-score=&\frac {2\times precision\times recall }{precision + recall}\tag{6}\end{align*}

where TP = true positive, TN = true negative, FP = false positive and FN = false negative.

Table \ref{t1-7-en} shows the performances of the implemented model on different datasets. From Table \ref{t1-7-en}, it is seen that after 50 epochs of training, the model achieved the highest training accuracy and loss of 99.81\% and 0.0015, validation accuracy and loss of 99.39\% and 0.0549 respectively in PlantVillage dataset (potato, corn, tomato). In the rice dataset, the model achieves the highest training accuracy of 99.94\%, training loss of 0.0030, validation accuracy of 99.66\%, and validation loss of 0.0041. To evaluate the robustness of our proposed model, we have considered the Cassava plant disease dataset in which the images were captured in real-time and in field conditions. Moreover, the images have complex backgrounds and having multiple leaves present on a single image. Compared with the PlantVillage and Rice plant datasets, the Cassava plant dataset gives less training and validation accuracy. The obtained training and validation accuracies are 98.17\% and 76.59\%, respectively. The cassava dataset’s performance accuracy drops compared to the plantvillage and rice dataset due to data imbalance, and the images in the dataset contain images with complex backgrounds.

\tab{t1-7}{0.7}{Summary of Deep Learning Based Implemented Methods}

After splitting the dataset into 80\%-20\% training and validation set we train the model upto 50 epoch and evaluate the performances on training and validation set. Figure \ref{f1-7-en}–​\ref{f1-9-en} shows the accuracy and loss for training and validation of the proposed model on PlantVillage, rice and cassava dataset respectively. From Figure \ref{f1-7-en}–​\ref{f1-9-en} it is seen that the proposed model gives satisfactory performances with fewer number of parameters.

\tab{f1-7}{0.7}{(a) Training \& validation accuracy (b) Training \& validation loss on PlantVillage dataset}

\tab{f1-8}{0.7}{(a) Training \& validation accuracy (b) Training \& validation loss on rice disease dataset}

\tab{f1-9}{0.7}{(a) Training \& validation accuracy (b) Training \& validation loss on cassava plant dataset}

We have also calculated the confusion matrix to evaluate the performance of the model. Table \ref{t1-8-en} shows the performances of the implemented model in terms of testing accuracy, precision, recall, and f1-score on three plant datasets. From Table \ref{t1-8-en}, it is seen that the Rice plant dataset gives the highest testing accuracy than PlantVillage and Cassava dataset. Figure \ref{f1-10-en} gives the performance metrics of the implemented model on different datasets. From Figure \ref{f1-10-en}, it is seen that the average prediction accuracy, precision, recall, and F1-score is more than 99\% on the rice and plantvillage dataset.

\fig{f1-10}{0.5}{Performance metric of proposed model on different dataset}

\tab{t1-8}{0.7}{Performance Metric of the Proposed Model on Testing Images}

\subsection{Comparison of Performances And Robustness of the Model}
To validate the stability of our proposed CNN model, we use k-fold cross-validation (5 fold) to evaluate the performances on the disease datasets. The disease datasets are divided randomly into five equal parts. Each part of the dataset is considered as the test set, and the remaining part of the dataset is considered as the training set. Based on the splitting of the dataset, we get the performance of the model on a different set of training and testing images. Table \ref{t1-9-en} shows the performances of the proposed model on each fold. From Table \ref{t1-9-en}, it is seen that there is not much performance deviation in each dataset. In the PlantVillage dataset, the accuracy ranges from 99.29\% to 99.37\%, in the Rice dataset, it ranges from 99.33\% to 99.66\% and in the Cassava dataset, the accuracy ranges from 76.42\% to 76.58\%. The result shows that our proposed CNN method has good stability in the identification of diseases and is consistent in each data split part.

\tab{t1-9}{0.7}{Result of Proposed CNN Based on k-Fold Cross Validation}

\subsection{Performance Comparison With Pre-Trained Network}
We have evaluated the performances of several pre-trained deep learning models such as VGG16, VGG19, InceptionV3, ResNet50, and DenseNet201 on three different datasets. The performance is compared with our proposed CNN model in terms of performance accuracy and training time. The performance comparison with other pre-trained deep learning models on three different datasets is shown in Table \ref{t1-10-en}. The accuracy defines the correctly identified classes of the images from the test image set. From the performance results, it is seen that our proposed model gives satisfactory performances on three different datasets (99.39\% on PlantVillage, 99.66\% on Rice, 76.59\% on imbalance cassava) with fewer parameters (428,100). Table \ref{t1-10-en} shows the time required to train the models per epoch on three different datasets. From Table \ref{t1-10-en}, it is seen that the required training time is much less in comparison with the other pre-trained networks in all three datasets as the number of layer used in our proposed model is less. The performance accuracies of the pre-trained deep learning models are less as the models uses the pre-trained weight where the models are trained on Imagenet Dataset. Due to the uses of pre-trained weight, these models did not achieve the optimal results. Whereas, the proposed model has an advantages of inception layer which has ability to extract better features as well as residual connection that removes vanishing gradient problem. Moreover, the uses of batch normalization and dropout increases the performance of model. Although the proposed model uses depthwise separable convolution which reduces the model parameter, also the number of layers used in this model is less. So the required time to train the model is much less as compared to pre-trained models. DenseNet201 requires more training time than the other deep learning models as the number of layers are more.

\tab{t1-10}{0.7}{Performance Comparison With Pre-Trained Network}

\subsection{Comparison With some Existing Literature}
To verify the performances of our proposed CNN model, we compare the performances of our proposed model with other deep learning models from the literature. Table \ref{t1-11-en} shows the performance comparison with some deep learning models. From Table \ref{t1-11-en}, it is seen that the proposed CNN model performs better. From Table \ref{t1-11-en}, we can also see that the proposed model requires much fewer parameter than other deep learning models. As the model uses fewer parameters, it requires less time to train the model compared to other deep learning models. From Table \ref{t1-11-en} it is also seen that the models were implemented on PlantVillage dataset where the images are captured in controlled environment. In this work, we have used three different dataset where rice disease images were captured in field condition and contains background image. The cassava dataset consist of field images along with more than one leaf in single image. Our proposed CNN model has advantages in terms of performance accuracy, parameter size, and training time.

\tab{t1-11}{0.7}{Performance Comparison With Different Deep Learning Models}

\section{Conclusion} \label{s5-en}
Deep learning is a recent and advanced technique in the field of image pattern recognition and much effective in the identification of diseases in plants. In this paper, we have proposed a novel CNN model based on the inception and residual connection that can effectively classify the diseases in plants. In addition, we have used depthwise separable convolution in inception architecture which reduces the computation cost by reducing the number of parameters by a margin of 70\%. Therefore, training the network requires much less time as compared to the standard CNN. The experimental result shows that the proposed model achieves higher performance accuracy. To evaluate the robustness of the model, we have used three different plant datasets. The testing accuracies of the proposed model is 99.39\%,99.66\% and 76.59\% on Plantvillage, Rice, and Cassava dataset, respectively. In  author recorded an accuracy rate of 52.87\% and 46.26\% using plain convolution neural network and deep residual neural network respectively on imbalance dataset. In athor achieved an accuracy rate of 80.6\% on balance dataset. In comparison with, our proposed model achieves a higher accuracy rate of 76.59\% on the imbalance cassava dataset. For the rice dataset Sethy et al. achieved an accuracy rate of 98.25\%, and our proposed model achieved higher accuracy of 99.66\% on the same dataset. In the future, we carry forward and try to investigate the performance of the proposed model on the different agricultural fields such as weed detection, pest identification, etc. Another future work includes the identification of plant diseases with different plant disease datasets with a different variety of images, different geographical regions. Using clustering based unsupervised technique in identification of diseases also an important aspects.


